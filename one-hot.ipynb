{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splines\n",
    "import torch\n",
    "import glob\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import encoded\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_dict=splines.mat_to_dict()\n",
    "spline_prims=splines.spline_dict(back_dict)\n",
    "images=splines.dict_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=encoded.OmniglotData(spline_prims)\n",
    "data_loader=DataLoader(dataset,batch_size=1,shuffle=False) # Using batchsize 1 to get single primitive at a time.\n",
    "# Create instances of models\n",
    "enc=encoded.encoder(latent_dim=2).to(device)\n",
    "dec=encoded.decoder(latent_dim=2).to(device)\n",
    "# Loading weights\n",
    "enc.load_state_dict(torch.load('./weights/2-d/encoder.pth',map_location=torch.device('cpu')))\n",
    "dec.load_state_dict(torch.load('./weights/2-d/decoder.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc9d6fc2e074eb6a7745d1fb63d17a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_2d=[]\n",
    "y_2d=[]\n",
    "for i,data in tqdm(enumerate(data_loader)):\n",
    "    data=data.to(device)\n",
    "    mu,var=enc(data)\n",
    "    lat_var=encoded.gen_latent(mu,var).squeeze().detach().cpu().numpy()\n",
    "    x_2d.append(lat_var[0].item());y_2d.append(lat_var[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "start=time.time()\n",
    "X=np.concatenate((np.reshape(x_2d,(-1,1)),np.reshape(y_2d,(-1,1))),axis=1)\n",
    "kmeans = KMeans(300, random_state=0)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "print(\"%d\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48762\n",
      "max:  299\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print('max: ',np.max(labels,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5505ab57c84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mprim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0;31m# yield torch.tensor(prim.reshape(-1)).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# yield torch.tensor(self.normalize(prim).reshape(-1)).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "data=images\n",
    "# images[lang][character][0]['image'][instance][0] -> (105,105) numpy ndarray\n",
    "for lang in data:\n",
    "    for char in data[lang]:\n",
    "        for inst in char:\n",
    "            vector=np.zeros((300))\n",
    "            for prim in inst[0]:\n",
    "                # yield torch.tensor(prim.reshape(-1)).float()\n",
    "                # yield torch.tensor(self.normalize(prim).reshape(-1)).float()\n",
    "                idx=labels[count]\n",
    "                vector[idx]+=1.\n",
    "                count+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now prepare the dataloader for the next model, for performing \n",
    "# supervised learning using the one-hot encoded vectors of a particular character.\n",
    "\n",
    "class OneHotDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable form dataset for providing one-hot encoded vectors for each character.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Just pass the spline_prims variable.\n",
    "        images (dict): Just pass the image dict.\n",
    "        labels (iterable type object): Labels for clustered data.\n",
    "        \n",
    "    Returns:\n",
    "        (Tensor) : One-hot encoded vector of length 300. (image,target)\n",
    "    \"\"\"\n",
    "    def __init__(self,data,images,labels,oh_len):\n",
    "        self.data=data\n",
    "        self.images=images\n",
    "        self.labels=labels\n",
    "        self.oh_len=oh_len\n",
    "    \n",
    "    def stream(self):\n",
    "        # spline_prims[lang][char][inst][0][primitive]\n",
    "        # images[lang][char][0]['image'][inst][0] -> (105,105) numpy ndarray\n",
    "        count=0\n",
    "        for lang in self.data:\n",
    "            for i,char in enumerate(self.data[lang]):\n",
    "                for j,inst in enumerate(char):\n",
    "                    vector=np.zeros((self.oh_len))\n",
    "                    for prim in inst[0]:\n",
    "                        # yield torch.tensor(prim.reshape(-1)).float()\n",
    "                        # yield torch.tensor(self.normalize(prim).reshape(-1)).float()\n",
    "                        idx=self.labels[count]\n",
    "                        vector[idx]+=1.\n",
    "                        count+=1\n",
    "                    # yield [image,vector]\n",
    "                    img=self.images[lang][i][0]['image'][j][0]\n",
    "                    yield [torch.tensor(img).unsqueeze(0).float(),torch.tensor(vector).float()]\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return int(19280) # Literally meaningless, hardcoded\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.stream()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_cluster(vector):\n",
    "    \"\"\"\n",
    "    Dummy function to just give the indices of the clusters.\n",
    "    \n",
    "    Arg:\n",
    "        vector: You know what it is.\n",
    "    \"\"\"\n",
    "    vector=vector.reshape(-1)\n",
    "    which=[]\n",
    "    idx=0\n",
    "    for _ in vector:\n",
    "        if vector[idx]==1:\n",
    "            which.append(idx)\n",
    "        if vector[idx]>1:\n",
    "            which.append('{0},special'.format(idx))\n",
    "        idx+=1\n",
    "    return which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=OneHotDataset(data=spline_prims,images=images,labels=labels,oh_len=300)\n",
    "data_loader=DataLoader(dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90, 106, 146, 253]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot=556\n",
    "for k,i in enumerate(data_loader):\n",
    "#     print(i[0].shape,i[1].shape)\n",
    "    if plot==k:\n",
    "        print(which_cluster(i[1]))\n",
    "        plt.imshow(i[0].squeeze().detach().cpu().numpy(),cmap='gray')\n",
    "        break\n",
    "#     print(type(i[0]),type(i[1]))\n",
    "#     print(which_cluster(i[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19280"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build the CNN now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=16)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
